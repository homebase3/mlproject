---
title: "Ames Housing dataset analysis"
description: |
  This analysis applies advanced tree-based learning algorithms to predict housing prices in the ![Ames housing dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview). It also identifies YY as the ZZ most important factors in driving housing prices.
author:
  - name: Justin Leder
output: 
  html_document:
    code_folding: hide
    highlight: espresso
---

```{r setup, include=FALSE}
# load packages
library(tidyverse)
library(plotly)
library(rmarkdown)
library(reticulate)

# set knitr
knitr::opts_chunk$set(echo = TRUE)
knitr::knit_engines$set(python = reticulate::eng_python)

#set options
options(stringsAsFactors = TRUE)
```

# Introduction

This kaggle competition asks the user to predict housing prices. The core dataset is show below. It carries 80 possible explanatory features for housing prices, split roughly evenly between numerical and categorical variables.

```{r load data, warning=FALSE,message=FALSE}
train <- read_csv('data/train.csv')
test <- read_csv('data/test.csv')
paged_table(train)
```

The first thing we notice is that there is a substantial amount of variation is our target variable, with a significant amount of leftward skew. We'll tackle this skew later on.

```{r paged.print=FALSE}
library(plotly)

density <- density(train$SalePrice)

fig <- plot_ly(x = ~density$x, y = ~density$y, type = 'scatter', mode = 'lines', fill = 'tozeroy')
fig <- fig %>% layout(xaxis = list(title = 'SalePrice'),
         yaxis = list(title = 'Density'))

fig
```

## Dataset preparation

There are three main issues that need to be addressed before we can train our ML models:

-   Missingness
-   Multicollinearity
-   Predictor normality

Let's tackle missingess first.

### Missingness

In the table below, we see missingness in \~1/4 (19/79) explanatory variables. \~90% of this missingness comes from 5 variables:

-   FireplaceQu
-   Fence
-   Alley
-   MiscFeature
-   PoolQC

```{r}
library(naniar)
miss_var_summary(train) %>% 
  mutate(cum_pct = cumsum(n_miss)/sum(n_miss)) %>%
  filter(n_miss>0) %>% 
  paged_table(.)
```

When we inspect the data description, we quickly see that almost all of this missingness is not true missingness, but rather tied to the way the data was encoded. For example:

-   ***PoolQC***: NA means "No pool"
-   ***FireplaceQu:*** NA means "No Fireplace"
-   ***Alley***: NA means "No alley access"
-   ***Fence***: NA means "No fence"
-   ***MiscFeature***: NA means "None"
-   ***GarageType, GarageFinish, GarageQual, GarageCond***: NA means "No garage"
-   ***BasmtQual, BsmtCond, BsmtFinType1, BsmtFinType2***: NA means "No basement"

```{r}
correct_NA = function(data) {
  col_list <- list()
  col_list[["PoolQC"]] <- "No pool"
  col_list[["FireplaceQu"]] <- "No fireplace"
  col_list[["Alley"]] <- "No alley"
  col_list[["Fence"]] <- "Fence"
  col_list[["MiscFeature"]] <- "None"
  for (col in c("GarageType","GarageFinish","GarageQual","GarageCond")) {
    col_list[[col]] <- "No garage"
  }
  for (col in c("BsmtQual","BsmtCond", "BsmtFinType1", "BsmtFinType2", "BsmtExposure")) {
    col_list[[col]] <- "No basement"
  }
  col_list[["LotFrontage"]] <- 0
  col_list[["GarageYrBlt"]] <- 0
  
  data %>% 
    tidyr::replace_na(col_list) %>%
    return(.)
}
train2 <- correct_NA(train) 
```

After making these adjustments, we see that the true missingness is actually quite limited (\~0.1% of the sample).

```{r}
train2 %>%
  miss_var_summary(.) %>%
  filter(n_miss >0) %>% 
  paged_table(.)
```

In theory, our tree-based algorithms may handle this directly, with minimal loss of generality. Unfortunately, when we run the same adjustment scheme on the test set, we find there is significantly more missingness (0.04%), including on several variables (e.g. MSZoning) that show no missingness in the train set.

```{r}
test2 <- correct_NA(test) 
test2 %>% 
  miss_var_summary(.) %>%
  filter(n_miss >0) %>% 
  paged_table(.)
```

To address this, we will use the `missForest` package, which implements random forest imputation in R. Its main advantage over its Python equivalent is that it handles categorical variables directly without the need to use oneHotEncoder or other dummification schemes. To use this package, we first need to convert our categorical variables into factors. We also need to ensure that our numerical variables are not just numerically encoded factors. After inspecting the `data_description.txt`, we perform the conversion below.

```{r}
factor_conversion <- function(data) {
  data %>% 
    mutate_at(vars(MSSubClass), as.character) %>% 
    mutate(across(where(is.character), as.factor))  %>% 
    as.data.frame(.) %>% 
    return(.)
}
train3 <- factor_conversion(train2)
test3 <- factor_conversion(test2)
```

Then we perform the imputation.

```{r cache=TRUE, message=FALSE, warning = FALSE, results = 'hide'}
library(missForest)
library(doParallel)
registerDoParallel(cores=6)
miss_train <- missForest(train3, parallelize = 'forests') 
train4 <- miss_train$ximp
miss_test <- missForest(test3, parallelize = 'forests')
test4 <- miss_test$ximp
```

We note that while the OOB error is very small in both cases, it is much larger in the test set than the train set:

```{r}
bind_rows(miss_train$OOBerror,miss_test$OOBerror) %>% 
  bind_cols(data.frame(Dataset = c("Train","Test")),.) %>% 
  paged_table()
```

### Multicollinearity

Given there are 80 explanatory variables in the dataset, it is unsurprising that multicollinearity is a signficiant concern. To size the issue, we must first define correlation in the context of a mix of categorical and numerical variables. We do so by leveraging the `GoodmanKruskal` package, which implements Goodman and Kruskal's tau measure. The tau measure is a association statistic that allows for (1) all variable types and (2) asymmetric relationships. It represents the proportion of variation in one variable that can be explained by another. Below is the cumulative distribution of tau measure across variable pairs:

```{r, cache = TRUE}
library(mltools)
library(GoodmanKruskal)
gktau <- train4 %>% 
  GKtauDataframe(.)
gkvec <- gktau %>% as.vector(.)
gkcdf <- gkvec[gkvec <= 1] %>% 
  empirical_cdf(.,ubounds=seq(0, 1, by=0.001)) 
plot_ly(x = ~gkcdf$UpperBound, y = ~gkcdf$CDF, type = 'scatter', mode = 'lines', fill = 'tozeroy') %>% 
   layout(xaxis = list(title = 'Goodman-Kruskal tau'),
         yaxis = list(title = 'Percent of variable relationships'))
```

As it turns out, multicollinearity is not as big of an issue here as would be feared. Only 33% of variable pairs explain more than 5% of the variation in one another. And only 17% of variable pairs explain more than 20% of the variance in one another.

To confirm this finding with a more intuitive association measure, let's consider the distribution of Pearson correlation coefficients among the numeric variables. Below is the analogous cumulative distribution plot:

```{r, cache = TRUE}
pears_vec <- lsr::correlate(train4) %>% 
  .$correlation %>% 
  as.vector(.) %>% 
  na.omit(.) %>% 
  abs(.)
pears_cdf <- pears_vec[pears_vec <= 1] %>% 
  empirical_cdf(.,ubounds=seq(0, 1, by=0.001))

plot_ly(x = ~pears_cdf$UpperBound, y = ~pears_cdf$CDF, type = 'scatter', mode = 'lines', fill = 'tozeroy') %>% 
   layout(xaxis = list(title = 'Absolute value of Pearson correlation coefficient'),
         yaxis = list(title = 'Percent of variable relationships'))
```

Only 25% of numerical variable pairs have a Pearson correlation coefficient \> 0.2.

This bodes well for the accuracy of our predictions, but likely means we will need to consider many variables in our model. In other words, features selection is likely to be particularly challenging. Rather than perform this in a separate step, this paper implements the [Boruta algorithm](https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a) to help automate feature selection within the larger model training process. That said, which features drive housing prices is a core insight of this paper, one that we'll return to after we fit our models.

### Target normality

While tree-based models can handle asymmetric target variables, transforming these variable can make models more accurate, especially in this case. The reason: our model will be evaluated on RMSE, so the long tail we are worried about (high-priced homes) are likely to have an outsize impact on the overall error figure. In this paper, we'll apply a `QuantileTransformer` using the `TransformedTargetRegressor` found in Scikit-learn.

## Model training

We are finally ready to train our model. We'll be applying three tree-based models:

-   ***XGBoost***: the classic Kaggle competition boosting algorithm
-   ***LightGBM***: Microsoft's gradient boosting algorithm known for its speed
-   ***CatBoost***: Yandex's offering known for its ability to seamlessly address categorical variables

### Dataset preparation

We must first split our train set into a train and test sets to prevent overtraining.

```{python}
from sklearn.model_selection import train_test_split
X = r.train4.iloc[:,1:-1] #dropping ID variable
X_hot = X
y = r.train4.iloc[:,-1:]
```

### Feature selection

Because hyperparameter tuning and other model steps are very computationally involved, this paper chooses to perfrom the feature selection step first. This is an imperfect choice, but should allow for much more reasonable calculation times on a personal computer. To do so, we'll leverage a relatively unknown package that allows for a quick implementation of XGBoost and other boosting algorithms into the Boruta framework. In particular, it automates categorical variable encoding, which significantly reduces user complexity.

### XGBoost

```{python, cache = TRUE}
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import OneHotEncoder, QuantileTransformer, quantile_transform
from sklearn.compose import TransformedTargetRegressor
import xgboost as xgb
from boruta import BorutaPy
import numpy as np
import pandas as pd

# X_hot = OneHotEncoder().fit_transform(X).toarray() #need to do encode categorical variables for XGBoost
# 
# xgb_model = xgb.XGBRegressor(tree_method = "hist", max_depth = 3)
# trans_model = TransformedTargetRegressor(regressor = xgb_model, transformer = QuantileTransformer())
br = BoostARoota(metric = "rmse")
X_xgb = pd.get_dummies(X)
y_trans = quantile_transform(r.train4.iloc[:,-1:])
br.fit(X_xgb,y_trans)
br.transform(X_xgb)
```

This variable selection returns \~40% of the total dummified variables. Interestingly though,

```{python, cache = TRUE}
len(br.keep_vars_)
len(np.unique([i.split("_")[0] for i in br.keep_vars_]))
```

### Hyperparameter tuning

Because of the computational complexity involved in tuning, we will use the \`optuna\` package to aid in our search. For each model, we will follow the following steps:

1.  Load relevant packages
2.  Initialize regressors
3.  Set ranges for hyperparameters
4.  Set up cross-validation scoring
5.  Functionalize steps 1-5 and pass to Optuna
6.  Get best parameters for each model from Optuna

Once we set hyperparameters, we will then run Boruta to help with feature selection
